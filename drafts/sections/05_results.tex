\chapter{Results}

This chapter presents the experimental results, including classification performance, confusion matrix analysis, statistical significance testing, feature importance analysis, error analysis, and comparison between speech tasks.

\section{Classification Performance}

\subsection{Overall Results}

Table \ref{tab:classification-results} summarises the classification performance for both speech tasks using 5-fold stratified cross-validation. The reported uncertainty ($\pm$) represents the standard deviation across the five cross-validation folds, reflecting variability in performance estimates.

\begin{table}[h]
\centering
\caption{Classification results by task and algorithm}
\label{tab:classification-results}
\begin{tabular}{lcccc}
\hline
\textbf{Task} & \textbf{Algorithm} & \textbf{Accuracy} & \textbf{F1 Score} \\
\hline
Reading & SVM & 71.5\% $\pm$ 6.8\% & 0.73 \\
Reading & Random Forest & 72.3\% $\pm$ 7.2\% & 0.74 \\
\hline
Interview & SVM & 82.0\% $\pm$ 5.2\% & 0.85 \\
Interview & Random Forest & \textbf{87.1\%} $\pm$ 4.8\% & \textbf{0.88} \\
\hline
\end{tabular}
\end{table}

For context, with approximately balanced classes (48\% HC, 52\% PT), chance-level accuracy is approximately 50\%. All classifiers substantially exceed this baseline, confirming that the acoustic features carry genuine discriminative information about depression status.

\subsection{Comparison with Prior Work}

These results compare favourably with Tao et al.'s published results on the same ANDROIDS corpus \cite{Tao2024}. Using deep learning approaches, they achieved 83.4\% accuracy on reading and 81.6\% on spontaneous speech. The present work achieves 87.1\% on interview speech using traditional machine learning---a 5.5 percentage point improvement over their spontaneous speech result, while maintaining full interpretability. This validates the argument that interpretable methods can be competitive with deep learning for this task.

\subsection{Key Finding: Interview Outperforms Reading}

The most striking result is the substantial performance gap between tasks. Interview speech yields approximately 15 percentage points higher accuracy than reading speech across both classifiers:

\begin{itemize}
    \item \textbf{Reading task:} 71.5--72.3\% accuracy
    \item \textbf{Interview task:} 82.0--87.1\% accuracy
\end{itemize}

This suggests that spontaneous speech contains richer markers of depression than controlled reading, likely because it captures a broader range of cognitive and emotional processes.

\subsection{Statistical Significance}

To confirm that this performance difference is genuine and not due to random variation, a two-proportion z-test was conducted comparing the Random Forest accuracy rates across tasks.

\begin{table}[h]
\centering
\caption{Statistical significance test: Reading vs Interview}
\label{tab:significance}
\begin{tabular}{lc}
\hline
\textbf{Metric} & \textbf{Value} \\
\hline
Reading accuracy & 72.3\% \\
Interview accuracy & 87.1\% \\
Difference & 14.8 percentage points \\
Z-statistic & 2.774 \\
P-value & 0.0055 \\
\hline
\end{tabular}
\end{table}

The p-value of 0.0055 is well below the conventional significance threshold of 0.05, indicating that \textbf{interview speech is statistically significantly better than reading speech for depression detection}.

\subsection{Classifier Comparison}

Random Forest marginally outperformed SVM on both tasks:
\begin{itemize}
    \item Reading: RF 72.3\% vs SVM 71.5\% (+0.8 percentage points)
    \item Interview: RF 87.1\% vs SVM 82.0\% (+5.1 percentage points)
\end{itemize}

The consistency of the interview $>$ reading pattern across both classifiers suggests this finding is robust to algorithm choice rather than an artefact of a particular method.

\section{Confusion Matrix Analysis}

Confusion matrices provide detailed insight into classification errors. Tables \ref{tab:cm-reading} and \ref{tab:cm-interview} show the confusion matrices for Random Forest on each task.

\begin{table}[h]
\centering
\caption{Confusion matrix: Reading task (Random Forest)}
\label{tab:cm-reading}
\begin{tabular}{lcc}
\hline
 & \textbf{Predicted HC} & \textbf{Predicted PT} \\
\hline
\textbf{Actual HC} & 36 & 18 \\
\textbf{Actual PT} & 13 & 45 \\
\hline
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Confusion matrix: Interview task (Random Forest)}
\label{tab:cm-interview}
\begin{tabular}{lcc}
\hline
 & \textbf{Predicted HC} & \textbf{Predicted PT} \\
\hline
\textbf{Actual HC} & 44 & 8 \\
\textbf{Actual PT} & 7 & 57 \\
\hline
\end{tabular}
\end{table}

Key observations:

\paragraph{Reading Task:}
\begin{itemize}
    \item False Positives (HC $\rightarrow$ PT): 18 (33\% of healthy controls)
    \item False Negatives (PT $\rightarrow$ HC): 13 (22\% of patients)
    \item The model shows a slight bias toward predicting depression
\end{itemize}

\paragraph{Interview Task:}
\begin{itemize}
    \item False Positives: 8 (15\% of healthy controls)
    \item False Negatives: 7 (11\% of patients)
    \item Substantially improved error rates with more balanced errors
\end{itemize}

The interview task confusion matrix demonstrates not only higher accuracy but also more balanced error types---both false positives and false negatives are low. This is clinically important: false positives cause unnecessary concern while false negatives mean missed cases.

\subsection{Detailed Classification Metrics}

Table \ref{tab:detailed-metrics} presents precision, recall, and F1 scores for each class.

\begin{table}[h]
\centering
\caption{Detailed classification metrics (Random Forest)}
\label{tab:detailed-metrics}
\begin{tabular}{llccc}
\hline
\textbf{Task} & \textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} \\
\hline
Reading & Healthy & 0.73 & 0.67 & 0.70 \\
Reading & Depressed & 0.71 & 0.78 & 0.74 \\
\hline
Interview & Healthy & 0.86 & 0.85 & 0.85 \\
Interview & Depressed & 0.88 & 0.89 & 0.88 \\
\hline
\end{tabular}
\end{table}

\section{Feature Importance Analysis}

The primary research question concerns which acoustic features are most predictive of depression. This section presents feature importance results ranked by Gini importance from the Random Forest classifier. Permutation importance was also computed as a robustness check; the two measures showed broad agreement in identifying the most predictive features, increasing confidence in these rankings.

\subsection{Reading Task Features}

Table \ref{tab:reading-features} shows the top 10 features for the reading task.

\begin{table}[h]
\centering
\caption{Top 10 predictive features for reading task}
\label{tab:reading-features}
\begin{tabular}{clc}
\hline
\textbf{Rank} & \textbf{Feature} & \textbf{Importance} \\
\hline
1 & slopeUV0-500\_amean & 0.050 \\
2 & mfcc1\_amean & 0.042 \\
3 & mfcc1\_stddevNorm & 0.041 \\
4 & loudnessPeaksPerSec & 0.036 \\
5 & StddevVoicedSegmentLengthSec & 0.032 \\
6 & mfcc1V\_amean & 0.029 \\
7 & slopeV500-1500\_stddevNorm & 0.027 \\
8 & VoicedSegmentsPerSec & 0.026 \\
9 & slopeV500-1500\_amean & 0.025 \\
10 & F2amplitudeLogRelF0\_amean & 0.025 \\
\hline
\end{tabular}
\end{table}

Importance scores range from 0.050 (top feature) to 0.025 (10th feature), indicating a relatively gradual decline. The top three features account for approximately 13\% of total importance, suggesting predictive power is distributed across multiple features rather than concentrated in one or two.

\subsubsection{Interpretation: Reading Task}

\paragraph{Spectral Slope (slopeUV0-500)} The top feature measures spectral tilt in unvoiced regions (0--500 Hz). Flatter slopes indicate breathier, less energetic voice quality---consistent with reduced vocal effort in depression.

\paragraph{MFCC1} The first mel-frequency cepstral coefficient captures overall spectral envelope shape. Both mean and variability appear in the top 10, suggesting depressed speech shows altered spectral characteristics.

\paragraph{Temporal Features} Loudness peaks per second and voiced segment variability reflect speech rhythm and prosodic patterns.

\subsection{Interview Task Features}

Table \ref{tab:interview-features} shows the top 10 features for the interview task.

\begin{table}[h]
\centering
\caption{Top 10 predictive features for interview task}
\label{tab:interview-features}
\begin{tabular}{clc}
\hline
\textbf{Rank} & \textbf{Feature} & \textbf{Importance} \\
\hline
1 & spectralFluxV\_stddevNorm & 0.069 \\
2 & hammarbergIndexV\_stddevNorm & 0.043 \\
3 & StddevUnvoicedSegmentLength & 0.038 \\
4 & MeanUnvoicedSegmentLength & 0.035 \\
5 & alphaRatioV\_stddevNorm & 0.035 \\
6 & mfcc1V\_amean & 0.033 \\
7 & mfcc1V\_stddevNorm & 0.032 \\
8 & loudness\_stddevRisingSlope & 0.030 \\
9 & loudness\_stddevFallingSlope & 0.026 \\
10 & logRelF0-H1-A3\_amean & 0.020 \\
\hline
\end{tabular}
\end{table}

Importance scores show a steeper decline than the reading task: the top feature (0.069) is nearly 3.5 times more important than the 10th feature (0.020). This suggests that a smaller set of features dominates predictive power for spontaneous speech.

\subsubsection{Interpretation: Interview Task}

\paragraph{Spectral Flux Variability} The most important feature measures frame-to-frame spectral change variability. Reduced variability indicates monotonous speech---a hallmark of depression-related flat affect.

\paragraph{Voice Quality Variability} The Hammarberg index and alpha ratio capture spectral balance related to breathiness and vocal strain. Notably, their \textit{variability} (standard deviation) rather than mean values drives predictions.

\paragraph{Pause Characteristics} Both mean and variability of unvoiced segment length rank highly. Unvoiced segments correspond to pauses and hesitations---clinically meaningful given the psychomotor retardation associated with depression.

\subsection{Key Insight: Variability Measures Dominate}

A striking pattern emerges: for interview speech, 7 of the top 10 features are variability measures (standard deviations), while for reading speech, only 2 of the top 10 are variability measures. This aligns with clinical characterisations of depressed speech as ``flat'' or ``monotonous''---not necessarily different in average properties, but reduced in \textit{dynamic modulation}.

\section{Task Comparison}

\subsection{Feature Overlap}

Comparing the top 10 features between tasks reveals minimal overlap: exactly \textbf{one feature} (mfcc1V\_amean) appears in both lists. This suggests that different acoustic markers are salient depending on speech context---a finding with implications for clinical system design.

\subsection{Dominant Feature Categories by Task}

\begin{table}[h]
\centering
\caption{Dominant feature categories by task}
\label{tab:feature-categories-comparison}
\begin{tabular}{lll}
\hline
\textbf{Category} & \textbf{Reading} & \textbf{Interview} \\
\hline
Spectral & Slope, MFCC1 & Flux, MFCC1V \\
Voice Quality & -- & Hammarberg, Alpha ratio \\
Temporal & Voiced segments & Unvoiced segments (pauses) \\
Prosodic & Loudness peaks & Loudness dynamics \\
\hline
\end{tabular}
\end{table}

\subsection{Clinical Interpretation}

The task-specific feature profiles suggest different cognitive mechanisms are captured:

\paragraph{Reading Task} Reading scripted text primarily reveals \textit{voice production} characteristics (spectral slope, MFCC) and basic rhythm. These may reflect the physiological and motor aspects of depression.

\paragraph{Interview Task} Spontaneous speech reveals \textit{cognitive and affective} processes---variable pausing (reflecting cognitive load and word-finding difficulty), reduced vocal dynamics (flat affect), and voice quality changes (emotional expression). The interview task places greater demands on executive function, emotional regulation, and language production---all impacted by depression.

This explains the superior classification performance on interview speech: it captures a broader range of depression-related processes.

\section{Error Analysis}

\subsection{Misclassification by Gender}

Analysis of misclassified samples revealed a pattern related to gender. To interpret this correctly, error \textit{rates} (not raw counts) must be considered given the gender imbalance in the dataset.

\begin{table}[h]
\centering
\caption{Misclassification rates by gender}
\label{tab:error-gender}
\begin{tabular}{llccc}
\hline
\textbf{Task} & \textbf{Gender} & \textbf{Errors} & \textbf{Total} & \textbf{Error Rate} \\
\hline
Reading & Female & 23 & 80 & 28.7\% \\
Reading & Male & 8 & 32 & 25.0\% \\
Interview & Female & 12 & 84 & 14.3\% \\
Interview & Male & 3 & 32 & 9.4\% \\
\hline
\end{tabular}
\end{table}

Female speakers show slightly higher error rates in both tasks (28.7\% vs 25.0\% for reading; 14.3\% vs 9.4\% for interview). However, these differences are modest---the raw count disparity (23 vs 8) is largely explained by the dataset being approximately 70\% female. This finding warrants further investigation but does not undermine the main results, since both genders show the same pattern of interview speech outperforming reading speech for depression detection.

\subsection{Validation Checks}

Two checks confirm the results are not artefactual:

\paragraph{No overfitting detected} Learning curves (Figure \ref{fig:learning-curves}) show training and validation scores converging, indicating the models generalise appropriately. Training accuracy reached approximately 92\% while validation accuracy stabilised around 87\% for the interview task.

\paragraph{Cross-method consistency} Both SVM and Random Forest produce the same pattern (interview $>$ reading), suggesting the finding is robust to algorithm choice.

\section{Visualisations}

Figure \ref{fig:feature-importance} presents the feature importance distributions for both tasks. The visual contrast is striking: reading task features show a relatively even distribution, while interview task features exhibit a steep drop-off after the top few features. Spectral flux variability visually dominates the interview chart, reinforcing its role as the primary predictive feature for spontaneous speech.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/advanced/feature_importance_comparison.png}
    \caption{Top 10 features by Gini importance: Reading task (left) vs Interview task (right). Note the steeper importance gradient for interview speech.}
    \label{fig:feature-importance}
\end{figure}

Figure \ref{fig:confusion-matrices} shows confusion matrix heatmaps for both tasks. The reading task matrix shows moderate off-diagonal values (errors), while the interview task matrix is visually cleaner---darker diagonal cells and lighter off-diagonal cells---immediately communicating the improved classification performance.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/advanced/reading_confusion_matrices.png}
    \includegraphics[width=0.48\textwidth]{figures/advanced/interview_confusion_matrices.png}
    \caption{Confusion matrices: Reading task (left) and Interview task (right). The interview task shows improved classification with fewer off-diagonal errors.}
    \label{fig:confusion-matrices}
\end{figure}

Figure \ref{fig:learning-curves} tracks training and cross-validation accuracy as training set size increases. Both tasks show the characteristic pattern of valid learning: training accuracy starts high and decreases slightly as training size grows, while validation accuracy increases and converges toward the training curve. The modest gap between curves at maximum training size ($\sim$5 percentage points) indicates appropriate generalisation without severe overfitting.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/advanced/reading_learning_curve.png}
    \includegraphics[width=0.48\textwidth]{figures/advanced/interview_learning_curve.png}
    \caption{Learning curves showing convergence of training and validation accuracy, indicating no severe overfitting.}
    \label{fig:learning-curves}
\end{figure}

\section{Summary of Findings}

\begin{enumerate}
    \item \textbf{Interview speech is significantly more informative:} 87.1\% vs 72.3\% accuracy (p = 0.0055), exceeding prior deep learning results on the same corpus
    
    \item \textbf{Different features matter for each task:} Only 1 of 10 top features overlaps between tasks
    \begin{itemize}
        \item Reading: Spectral slope, MFCC, rhythm
        \item Interview: Spectral dynamics, pausing, voice quality variability
    \end{itemize}
    
    \item \textbf{Variability measures are key for interview speech:} 7 of 10 top features are standard deviations, consistent with ``flat'' depressed speech
    
    \item \textbf{Pausing behaviour is highly informative:} Unvoiced segment features rank 3rd and 4th for interview speech
    
    \item \textbf{Confusion matrices show balanced errors for interview:} Both false positives (8) and false negatives (7) are low
    
    \item \textbf{Gender shows modest effect on error rates:} Female speakers have slightly higher error rates, though the difference is small
\end{enumerate}

\subsection{Limitations}

Feature importance rankings identify \textit{which} features are predictive but do not establish whether differences between features are statistically significant. Future work could apply permutation tests or bootstrap confidence intervals to determine whether, for example, the top-ranked feature is significantly more important than the second-ranked feature. For present purposes, the consistency between Gini and permutation importance measures provides reasonable confidence in the top-ranked features.
