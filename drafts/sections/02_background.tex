\chapter{Background}

This chapter establishes the theoretical and technical foundations necessary for understanding the present research. It begins by examining the clinical context of depression and its effects on speech production, then explores how these speech changes can be measured through acoustic features, before reviewing existing computational approaches and identifying the gap this work addresses.

\section{Depression and Its Effects on Speech}

\subsection{The Clinical Context}

Major Depressive Disorder (MDD) is characterised by persistent feelings of sadness, hopelessness, and diminished interest in activities. Beyond its emotional symptoms, depression profoundly affects cognitive and motor functioning---effects that manifest in observable changes to speech production.

The neurobiological basis for these speech changes is well-established. Depression is associated with dysfunction in prefrontal cortical regions responsible for executive control, as well as disruption to dopaminergic pathways that regulate motor initiation and reward processing \cite{Sobin1997}. Psychomotor retardation---the slowing of physical and cognitive processes---is a core feature of melancholic depression, affecting approximately 70\% of hospitalised patients. This retardation directly impacts the motor planning and execution required for fluent speech production.

At the cognitive level, depression impairs working memory capacity and attentional control \cite{Joormann2011}. Speech production is a demanding cognitive task: Levelt's influential model identifies multiple processing stages including conceptualisation, formulation, and articulation, all operating in parallel and requiring continuous monitoring \cite{Levelt1989}. When cognitive resources are depleted by rumination or slowed by psychomotor retardation, these processes become less efficient, manifesting as increased hesitations, longer pauses, and reduced fluency.

\subsection{How Depression Manifests in Speech}

The observable speech changes in depression can be understood through three interconnected mechanisms.

\paragraph{Prosodic flattening} reflects the emotional blunting characteristic of depression. Healthy emotional expression involves dynamic modulation of pitch, loudness, and timing---what linguists term prosody. Depressed individuals show reduced emotional reactivity \cite{Bylsma2008}, which manifests as diminished prosodic variation: narrower pitch range, reduced loudness dynamics, and more monotonous delivery. Importantly, this flattening appears to be involuntary; speakers cannot easily mask it, making it a potentially robust biomarker.

\paragraph{Temporal disruption} arises from psychomotor retardation and cognitive load. The production of spontaneous speech requires real-time lexical retrieval, syntactic planning, and articulatory coordination. When these processes are slowed, the result is increased pause duration, lower speech rate, and more frequent hesitations. Cannizzaro et al. found that pause-related features were among the strongest acoustic correlates of depression severity \cite{Cannizzaro2004}.

\paragraph{Voice quality changes} reflect alterations to laryngeal function. Depression is associated with increased muscle tension and changes to respiratory control, which affect vocal fold vibration patterns. These changes manifest as increased jitter (cycle-to-cycle pitch variation), shimmer (amplitude variation), and breathiness. Ozdas et al. demonstrated that voice quality measures could distinguish depressed from non-depressed speakers with reasonable accuracy, though results have been less consistent across studies than prosodic measures \cite{Ozdas2004}.

A critical observation from the literature is that \textbf{variability measures often outperform means}. It is not simply that depressed speakers have lower average pitch or speak more slowly; rather, they show reduced \textit{dynamic range}---less variation around whatever their baseline might be. This insight, while present in the clinical literature, has been underexplored in computational approaches that often focus on mean values.

\section{Acoustic Feature Extraction}

\subsection{From Speech Signal to Measurable Features}

To analyse speech computationally, the raw audio signal must be transformed into a structured representation. This process involves extracting acoustic features---numerical descriptors that capture relevant properties of the speech signal. The choice of features is consequential: different features capture different aspects of speech, and their relevance to depression detection varies.

\subsection{Prosodic Features}

\textbf{Fundamental frequency (F0)}, corresponding to vocal fold vibration rate and perceived as pitch, is the most studied feature in depression research. F0 is typically extracted using autocorrelation or cepstral methods, producing a time series that can be summarised through statistical functionals: mean, standard deviation, percentiles, and slope measures. The theoretical motivation is clear: F0 variation reflects emotional expressiveness, and reduced variation is a hallmark of depressive flattening.

However, F0-based features have limitations. Extraction algorithms can be unreliable in noisy conditions or with certain voice types, and cross-speaker normalisation is challenging due to large individual differences in baseline pitch. Some researchers argue that F0 variability, while theoretically motivated, has shown inconsistent results precisely because of these measurement challenges \cite{Cummins2015}.

\textbf{Loudness and energy features} capture vocal intensity and its dynamics. Root mean square energy provides an overall measure, while loudness slope features (both rising and falling) characterise the dynamic envelope of speech. These features are more robust to extraction than F0 but potentially confounded by recording conditions and speaker-microphone distance.

\subsection{Spectral Features}

\textbf{Mel-Frequency Cepstral Coefficients (MFCCs)} are the standard spectral representation in speech processing. The computation involves applying a mel-scaled filterbank to the power spectrum, taking the logarithm, and applying a discrete cosine transform. The resulting coefficients approximate human auditory perception and capture vocal tract configuration.

MFCCs are ubiquitous in speech technology but their relevance to depression detection deserves scrutiny. They were developed for speech recognition, where the goal is capturing phonetic content. For depression detection, their utility is empirical rather than theoretically motivated---they work reasonably well, but it is unclear which aspects of the spectral shape they capture are actually relevant to depressive speech changes. This ambiguity makes interpretation difficult.

\textbf{Spectral flux} measures frame-to-frame spectral change, capturing the dynamic modulation of the voice. Unlike MFCCs, which describe static spectral shape, spectral flux characterises how that shape evolves over time. This may be more directly relevant to the reduced dynamic variation observed in depression.

\textbf{Voice quality features} including the Hammarberg index (ratio of energy below vs above 2kHz) and alpha ratio (similar spectral balance measure) capture characteristics like breathiness and vocal strain. These have clear physiological interpretations related to laryngeal function.

\subsection{The eGeMAPS Feature Set}

Recognising the proliferation of ad-hoc feature sets in affective computing, Eyben et al. proposed the extended Geneva Minimalistic Acoustic Parameter Set (eGeMAPS): a standardised set of 88 features designed specifically for affective computing and clinical speech analysis \cite{Eyben2016}.

The eGeMAPS set was motivated by several considerations. First, \textbf{parsimony}: rather than thousands of features requiring aggressive dimensionality reduction, 88 features provide comprehensive coverage while remaining manageable. Second, \textbf{interpretability}: each feature has a defined acoustic meaning, unlike the opaque representations learned by neural networks. Third, \textbf{standardisation}: using a common feature set enables comparison across studies.

The 88 features span frequency (F0 statistics and dynamics), energy (loudness and its variations), spectral (MFCCs 1-4, spectral flux, formants), and temporal (voiced/unvoiced segment statistics) domains. Functionals---statistical summaries computed over the entire recording---transform frame-level measurements into fixed-length vectors.

This work adopts eGeMAPS for several reasons aligned with the research question. The focus on feature interpretability requires features with clear meanings; eGeMAPS provides this. The goal of identifying \textit{which} features matter requires a manageable set that can be analysed individually; eGeMAPS is sized appropriately. And the use of a standard set enables comparison with prior work on the same corpus.

\section{Computational Approaches to Depression Detection}

\subsection{Traditional Machine Learning}

Early computational work on depression detection employed classical machine learning algorithms with handcrafted features. Support Vector Machines (SVMs) were particularly popular, performing well with high-dimensional feature vectors and limited training data. Random Forests offered the additional advantage of built-in feature importance estimates through Gini impurity or permutation-based measures.

These approaches achieved reasonable accuracy---typically 65-80\% on binary classification tasks---while maintaining interpretability. A trained SVM or Random Forest can be interrogated: which features received high weights? Which drove particular predictions? This transparency is valuable for clinical applications where explanations matter.

\subsection{Deep Learning Approaches}

The deep learning revolution reached depression detection around 2016. Convolutional Neural Networks (CNNs) applied to spectrograms, Recurrent Neural Networks (RNNs) modelling temporal dynamics, and attention mechanisms focusing on relevant speech segments all showed improvements over traditional methods.

Ma et al. demonstrated that CNNs could learn depression-relevant representations directly from spectrograms, bypassing handcrafted features entirely \cite{Ma2016}. Subsequent work, including Tao et al.'s Multi-Local Attention approach, achieved state-of-the-art results on benchmark datasets \cite{Tao2023MLA}.

However, these improvements came at a cost. Neural networks are notoriously difficult to interpret. While post-hoc explanation methods like SHAP and attention visualisation offer some insight, they do not provide the direct feature-level understanding that traditional methods afford. For a researcher asking ``which acoustic properties of speech are affected by depression?'', a neural network's answer is unsatisfyingly opaque.

This trade-off between accuracy and interpretability is central to the present work. The question is not simply ``can we detect depression?'' but ``what can detection tell us about how depression affects speech?''

\section{Related Work and Datasets}

\subsection{The AVEC Challenge Series}

The Audio/Visual Emotion Challenge (AVEC) series established benchmarks for affective computing, including depression detection. AVEC 2016, 2017, and 2019 featured depression sub-challenges using the DAIC-WOZ corpus \cite{Gratch2014}.

Examining the winning approaches reveals important patterns. The 2016 winner combined audio, video, and text modalities, achieving 4.99 MAE on PHQ-8 prediction. Subsequent challenges showed similar trends: multimodal approaches consistently outperformed unimodal ones, often by substantial margins. This raises questions about speech-only approaches: is acoustic information sufficient, or are visual and linguistic cues essential?

The improvement from multimodality is not uniform across features. Analysis of challenge results suggests that acoustic features contribute most to detecting severe depression, while linguistic features (word choice, response patterns) may better capture mild cases. This observation has implications for system design but has received limited attention in the literature.

\subsection{The DAIC-WOZ Corpus}

The Distress Analysis Interview Corpus (DAIC-WOZ) has been the primary benchmark for depression detection research \cite{Gratch2014}. It contains clinical interviews with participants who completed PHQ-8 self-report questionnaires.

Despite its influence, DAIC-WOZ has limitations relevant to this work. First, labels are self-reported questionnaire scores, not clinical diagnoses---introducing potential noise from response biases. Second, all speech is spontaneous interview responses; there is no controlled reading task for comparison. Third, access is restricted, limiting reproducibility.

\subsection{The ANDROIDS Corpus}

Tao et al. introduced the ANDROIDS corpus specifically to address limitations in existing datasets \cite{Tao2023ANDROIDS}. Three features make it particularly suitable for the present research.

\textbf{Clinical labels}: Unlike DAIC-WOZ's self-report scores, ANDROIDS participants were labelled based on psychiatric assessment by clinicians. This provides more reliable ground truth, though the binary healthy/depressed distinction loses severity information.

\textbf{Dual speech tasks}: Critically, ANDROIDS includes both reading and spontaneous interview tasks from the same participants. This enables direct comparison of how depression manifests across speech modalities---a comparison impossible with DAIC-WOZ. Reading provides controlled linguistic content, isolating acoustic characteristics; interviews capture naturalistic speech with all its cognitive demands.

\textbf{Public availability}: ANDROIDS is freely accessible for research, enabling reproducibility.

Tao et al.'s own analysis of ANDROIDS achieved 83.4\% accuracy on reading and 81.6\% on spontaneous speech using deep learning \cite{Tao2024}. However, their focus was on detection accuracy, not feature analysis. They did not systematically investigate which acoustic features drove their model's predictions or whether the same features mattered for both tasks.

\subsection{The Gap: Feature Interpretability}

Surveying the literature reveals a consistent pattern: most work prioritises detection accuracy over understanding. Researchers report overall accuracy, F1 scores, and comparisons to baselines, but rarely analyse \textit{which features} contribute most to predictions or \textit{why}.

This gap exists for understandable reasons. The field has been driven by challenge benchmarks that reward accuracy, not interpretability. Deep learning methods that dominate leaderboards are inherently opaque. And rigorous feature importance analysis requires careful methodology that adds complexity beyond simply training a classifier.

Yet the gap has costs. Clinicians cannot trust systems they do not understand. The scientific goal of understanding depression's effects on speech is not served by black-box predictions. And practical systems cannot be optimised without knowing which features matter.

The ANDROIDS corpus, with its dual tasks and clinical labels, provides an ideal testbed for addressing this gap. By analysing feature importance across reading and spontaneous speech, we can ask: which acoustic features are most predictive of depression? Do the same features matter in controlled versus naturalistic speech? What does this reveal about how depression affects speech production?

\section{Summary}

Depression affects speech through multiple mechanisms: psychomotor retardation disrupts timing, emotional blunting reduces prosodic variation, and altered laryngeal function changes voice quality. These effects can be captured through acoustic features spanning prosodic, spectral, and temporal domains.

Computational approaches have achieved reasonable detection accuracy, with deep learning methods currently dominant. However, the field has prioritised accuracy over interpretability, leaving important questions unanswered about which features drive predictions and why.

The ANDROIDS corpus offers unique advantages for addressing these questions: clinical labels, dual speech tasks, and public availability. This work uses ANDROIDS with interpretable machine learning methods to analyse feature importance, directly addressing the gap in current research.
